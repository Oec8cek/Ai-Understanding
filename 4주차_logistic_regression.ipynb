{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Oec8cek/Ai-Understanding/blob/main/4%EC%A3%BC%EC%B0%A8_logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFwvBXZIHqxZ"
      },
      "source": [
        "github에 공개된 정보 활용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxS7M_m5nEWx"
      },
      "outputs": [],
      "source": [
        "#코랩 탭의 파일구조 내에서 위에서 복제한 실습 자료가 있는 폴더로 이동한다.\n",
        "%cd /content/logisticregression\n",
        "# 나이(Age)와 급여(Salary)에 따른\n",
        "# 상품 구매 여부(Purchased)(구매했는가 1 or 구매하지 않았는가? 0)\n",
        "# 를 판단하기 위한 정형데이터와 이진 분류 모델을 실습한다.\n",
        "\n",
        "\n",
        "## 런타임을 전부 실행하고\n",
        "## 왼쪽 하단의 폴더 아이콘을 클릭하여  /content/logisticregression 폴더 경로에 Social_Network_Ads.csv 파일을 더블클릭해보자.\n",
        "## 오른쪽에 데이터(소셜네트워크관련)를 확인할 수 있다.\n",
        "## 행과 열에 어떤 카테고리들이 특징벡터로, 실제값으로 각각 활용되는지 확인해보자."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCTiqXzgiaU7"
      },
      "source": [
        "# 로지스틱 회귀 (logistic regression) (실습)\n",
        "\n",
        "### [1] 실습 진행 관련 설명\n",
        "로지스틱 회귀에 대한 실습을 진행하고자 한다. 실습은 크게 두가지로 구성되어 있다.\n",
        "\n",
        "- sklearn 라이브러리를 이용한 방법\n",
        "- 직접 함수를 구현하는 방법\n",
        "\n",
        "처음에는 sklearn 라이브러리를 이용한 방법을 실습하고 그 이후에 직접 함수를 구현하여 실습을 진행한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCuGUgRwilNi"
      },
      "source": [
        "## [2] 코드 구현의 흐름\n",
        "\n",
        "\n",
        "- (1) (데이터 측면) 데이터 불러오기 및 특성 스케일링\n",
        "\n",
        "- (2) (데이터 측면) 데이터에서 훈련데이터와 테스트 데이터로 구분하고\n",
        "\n",
        "- (3) (모델 측면) sklearn 라이브러리를 이용하여 로지스틱 회귀 모델 불러오기. 또는 직접 함수 구현하기 (로지스틱회귀 모델 정의).\n",
        "\n",
        "- (4) (학습 과정) sklearn 라이브러리를 이용하여 모델 학습하기. 또는 직접 구현하기 (Loss 정의 & Sigmoid).\n",
        "\n",
        "- (5) (성능 평가) 테스트 데이터를 이용하여 학습된 모델에 대한 성능 평가하기. (정확도)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBRiMpRTIREK"
      },
      "source": [
        "이번 실습에서 활용한 데이터는 \"User ID\", \"Gender\", \"Age\", \"EstimatedSalary\", \"Purchased\"의 컬럼으로 구성되어 있으며, 학습 과정에서는 \"Age\"와 \"EstimatedSalary\"를 특징으로 활용(X = dataset.iloc[:, [2, 3]].values)\n",
        "\n",
        "종속변수에 해당하는 값은 \"Purchased\" 컬럼(y = dataset.iloc[:, 4].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDrZBGBxLD4x"
      },
      "source": [
        "(참고)\n",
        "\n",
        "**Feature scaling**\n",
        "\n",
        "이번 실습에서는 주어진 데이터의 값을 변환하는 과정이 포함되어 있다.\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "sc = StandardScaler()\n",
        "\n",
        "라는 2줄의 코드를 보면 (1) sklearn의 preprocessing 패키지로부터 \"StandardScaler\"를 불러온 후 (\"from sklearn.preprocessing import StandardScaler\") (2) StandardScaler()에 \"sc\"라는 이름을 부여하여 사용한다(\"sc = StandardScaler()\").\n",
        "\n",
        "StandardScaler는 \"x\"라는 값을 \"z\"라는 값으로 변환하는데 변환 공식은 z=(x-u)/s (여기서 \"u\"는 학습데이터 관측값의 평균, \"s\"는 학습데이터 관측값의 표준 편차)으로 표현된다.인공지능입문에서는 StandardScaler를 비롯한 feature scaling을 하면 학습에 큰 도움을 준다고 이해하여도 충분하다.\n",
        "\n",
        "**주의**\n",
        "\n",
        "X_train = sc.fit_transform(X_train)\n",
        "\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "위의 두 줄의 코드를 살펴보면 X_train에 대해서는 sc의 \"fit_transform()\"을 사용하고 X_test에 대해서는 sc의 \"transform()\"을 사용한다. \"fit_transform()\"을 먼저 X_train에 적용하면 X_train의 평균과 분산값을 확보한 후 특징 값을 변화시킨다. \"transform()\"은 \"fit_transform()\"에서 확보한 X_train의 평균과 분산값을 X_test에 적용하여 특징 값을 변화시킨다. 훈련데이터와 테스트데이터에 대하여 feature scaling을 적용할 때는 동일한 인자를 적용하여 특징 값을 변화시키도록 주의하자."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa6mgQRDjQEp"
      },
      "source": [
        "## (첫번째 실습) sklearn 라이브러리를 이용한 방법\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IqzG38IPj8H"
      },
      "source": [
        "**모델 학습과 테스트**\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "classifier = LogisticRegression(random_state = 0)\n",
        "\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "위 세줄의 코드에서는 (1) LogisticRegression 모델을 불러온 후(\"from sklearn.linear_model import LogisticRegression\") (2) 우리가 사용할 LogisticRegression 모델에 \"classifier\"라는 이름을 부여하고(\"classifier = LogisticRegression(random_state = 1)\") (3) 모델을 학습시킨다(\"classifier.fit(X_train, y_train)\")\n",
        "\n",
        "sklearn의 LogisticRegression을 사용할 때는 다양한 인자를 함께 설정할 수 있는데(인자의 자세한 설명은 [표준문](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)서를 참고), random_state의 효과에 대해서는 표준문서의 설명을 알아보자.\n",
        "\n",
        "학습이 끝난 모델의 테스트는 \" predict()\"함수를 사용한다(y_pred = classifier.predict(X_test))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_nBmTUYDTSH"
      },
      "source": [
        "**분류의 성능평가지표**\n",
        "\n",
        "분류 작업을 수행하는 모델의 성능을 평가할 때는 다양한 성능측도를 사용하는데 우선은 Accuracy와 F1 score를 알아보자.\n",
        "\n",
        "1. Accuracy라는 평가지표는 테스트 데이터에서 모델이 클래스 레이블을 정확하게 예측한 비율을 표시한다.\n",
        "\n",
        "2. Precision은 Positive와 Negative라는 두 개의 클래스에 대한 이진 분류 문제에서 모델이 Positive클래스에 속한다고 예측한 관측값 중 실제로 Positive 클래스에  속한 관측값의 비율을 표시한다.\n",
        "\n",
        "3. Recall은 Positive 클래스에 속하는 관측값 중 모델이 Positive라고 예측한 관측값의 비율을 표시한다.\n",
        "\n",
        "4. F1 score는 2*(Precision * Recall)/(Precision + Recall)을 의미"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxB6EEWMnHQV"
      },
      "outputs": [],
      "source": [
        "# 로지스틱 회귀(Logistic Regression)\n",
        "# 필수 라이브러리 가져오기 : 함수와 기능들의 묶음.\n",
        "import numpy as np   # 넘파이, 계산, 수학을 위한 라이브러리\n",
        "import matplotlib.pyplot as plt  # 맷플롯립, 시각화를 위한 라이브러리\n",
        "import pandas as pd  # 판다스, 데이터를 불러오기위한 라이브러리\n",
        "import sklearn  # 사이킷런, 회귀 분류 등 인공지능 모델을 간단한 함수만으로 구현 가능한 라이브러리\n",
        "\n",
        "# (1) (데이터 측면) 데이터 불러오기 (Importing the datas) 및 스케일링\n",
        "dataset = pd.read_csv('Social_Network_Ads.csv')\n",
        "## 데이터셋을 판다스의 read_csv 함수로 불러온다.\n",
        "## csv 란? 테이블이나 스프레드시트로 데이터를 설명하는 정형데이터 표현 형식\n",
        "X = dataset.iloc[:, [2, 3]].values\n",
        "# iloc은 넘파이의 인덱싱/슬라이싱 기능과 동일하다. [행, 열] 개념에 유의\n",
        "y = dataset.iloc[:, 4].values\n",
        "## 2번 Age 나이, 3번 Salary 급여 // 4번은 Purchased 상품구매여부 의 열(column) 인덱스를 불러오는 표현이다.\n",
        "\n",
        "\n",
        "\n",
        "# 특성 스케일링 : 스케일링을 통해 데이터 분포를 정규화 할 수 있다.\n",
        " ## https://wikidocs.net/87252 사이트에서 스케일링을 통해 데이터 분포가 어떻게 변화하는지 확인하라.\n",
        " ## 정규화를 통해서 데이터로부터 도출되는 확률분포의 추정능력이 향상되고, 연산량도 줄어들게 되어 많은 이점이 있다.\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X = sc.fit_transform(X)\n",
        " ##. 학습데이터에 대해서 \"fit_transform()\" 평균과 분산을 표준정규분포 형태로 바꿔준다.\n",
        " ##. https://deepinsight.tistory.com/165\n",
        " ##. 실제로 많이 사용되지만 데이터의 분산으로 인한 편향을 없애주는 좋은 기능이다 정도로 알아두자.\n",
        "\n",
        "# (2) (데이터 측면) 훈련 데이터셋과 테스트 데이터셋을 구분하기\n",
        "from sklearn.model_selection import train_test_split\n",
        " ##. split은 쪼개다.라는 뜻. 사이킷런으로부터 학습데이터와 검증데이터를 나눈다.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 1)\n",
        " ##. test_size=0.25 // 75%, 25% 비율로 학습용데이터와 검증용데이터를 나눈다. 즉, 300개 / 100개\n",
        " ##. random_state // 회귀에서는 0이었지만 난수를 1로 설정함으로써 데이터의 순서에의한 편향성을 제거하도록 랜덤하게 데이터 순서를 섞어서 입력으로 사용한다.\n",
        "\n",
        "\n",
        "# (3) (모델 측면) 로지스틱 회귀 모델 불러오기\n",
        " ##. 로지스틱 회귀는 사이킷런을 활용할때 모델의 선형판별함수, 로지스틱함수, 예측, 이진분류 손실함수와 경사하강법까지\n",
        " ##. 전부 LogisticRegression.fit() // 함수 하나로 모든 프로세스가 진행가능하다.(매우 편리)(두번째 실습 직접구현과 비교해보라.)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier = LogisticRegression()\n",
        "\n",
        "# (4) (학습 과정) 모델 학습 하기\n",
        "# 학습과정에서는 Train 데이터만! 사용된다.\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "print(classifier.coef_)\n",
        "print(classifier.intercept_)\n",
        " ## coef_ : 최종 업데이트가 완료된 가중치 w1, w2 를 의미한다.\n",
        " ## intercept_ : 최종 업데이트가 완료된 가중치 w0 를 의미한다.\n",
        "\n",
        "# (5) (성능 평가) 학습된 모델을 대상으로 테스트 데이터로 평가하기. (Predicting the Test set results)\n",
        "# 성능평가 과정에서는 Test 데이터만! 사용된다.\n",
        "# 테스트 데이터에 대하여 예측값 확보\n",
        "y_pred = classifier.predict(X_test)\n",
        " ## 이번엔 테스트 데이터에 대해서 프로세스를 진행하는데 이때는 predict()함수가 손실과 경사하강법을 제외하고\n",
        " ## 모델의 예측값만 계산해준다.\n",
        "\n",
        "#분류 성능 측도\n",
        "print(sklearn.metrics.accuracy_score(y_test, y_pred))\n",
        " ## Accuracy 우리가 통상 이해하는 모델의 신뢰도 (성공의 확률/실패의 확률)을 의미한다.\n",
        "print(sklearn.metrics.f1_score(y_test, y_pred))\n",
        " ## F1 score는 accuracy와 함께 모델의 신뢰도를 측정하는 수치(점수)의 종류 중 하나이다.\n",
        " ## 학습데이터 만으로 분류 확률이 잘 나온다고해서 새로운 데이터가 무수히 등장할 산업현장에 AI모델을 투입할 수는 없지 않은가! 따라서 신뢰도 검증이 필요하다.\n",
        " ## https://www.linkedin.com/pulse/precision-recall-f1-score-object-detection-back-ml-basics-felix\n",
        " ## 더 알고 싶다면, 위 사이트의 내용을 참고해보세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlgwad8PRY-P"
      },
      "source": [
        "- 경고무시\n",
        "\n",
        "프로그램을 작성할 때 에러 메시지와 경고메시지는 매우 중요한 역할을 하지만, 경고 메시지의 경우 출력 화면에 부가 정보를 대량으로 표시하여 결과 확인을 어렵게 하기도 한다. 이런 경우 파이썬의 경고 메시지를 무시하도록 설정할 수 있는데, 경고 메시지를 무시하고 싶으면 다음 코드를 추가한다.\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkAMarQE3ilm"
      },
      "source": [
        "**결과 시각화**\n",
        "\n",
        "코드에서는 ListedColormap과 등고선플롯(contourf)을 이용하여 결과를 가시화하고 있다. 인공지능입문에서 소개되는 matplotlib의 다양한 기능을 활용하여 결과를 직관적으로 표시할 수 있도록 화면에 그려지는 결과를 잘 살펴보자."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqYViBsNlSIZ"
      },
      "outputs": [],
      "source": [
        "# 경고 메시지 무시\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "### 훈련 결과 가시화\n",
        "from matplotlib.colors import ListedColormap\n",
        "X_set, y_set = X_train, y_train\n",
        "  ## 맷플롯립(시각화 도구) 함수에 학습데이터를 입력시킨다.\n",
        "\n",
        "#격자 형태 그리드\n",
        "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
        "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
        "  ## 메쉬그리드는 그래프의 밑그림을 그려준다.\n",
        "\n",
        "#등치선 표현 함수\n",
        "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
        "\n",
        "  ## 컨투어에프 함수는 초평면, 선형판별함수를 그려준다.\n",
        "\n",
        "plt.xlim(X1.min(), X1.max())\n",
        "plt.ylim(X2.min(), X2.max())\n",
        "  ## x, y 값의 최대, 최소 표현 범주를 정의한다.\n",
        "for i, j in enumerate(np.unique(y_set)):\n",
        "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
        "                c = ListedColormap(('red', 'green'))(i), label = j)\n",
        "  ## 빨강, 초록으로 특징벡터 점들의 클래스 범주를 나누어준다.\n",
        "\n",
        "plt.title('Logistic Regression (Training set)') # 그래프 제목\n",
        "plt.xlabel('Age') # x축 라벨\n",
        "plt.ylabel('Estimated Salary') # y축 라벨\n",
        "plt.legend() # 범례\n",
        "plt.show() # 이모든걸 아래 출력하시오."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYuNagCqG2T3"
      },
      "source": [
        "- 학습데이터로 학습된 결과 0일 확률은 붉은 색,\n",
        " 1일 확률은 초록 색으로 나타냈으며 초평면 대각선이 이러한 범주를 나누고, 서로다른 색상으로 드러나는 점들은 전부 모델이 예측에 실패한 데이터들이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQri9t81oAh3"
      },
      "outputs": [],
      "source": [
        "### 테스트 결과 시각화\n",
        "from matplotlib.colors import ListedColormap\n",
        "X_set, y_set = X_test, y_test\n",
        " ## 이번엔 테스트, 즉 검증 데이터로 시각화를 해보자 위에서 사용한 맷플롯립코드와 동일하다.\n",
        "\n",
        "#격자 형태 그리드\n",
        "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
        "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
        "#등치선 표현 함수\n",
        "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
        "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
        "plt.xlim(X1.min(), X1.max())\n",
        "plt.ylim(X2.min(), X2.max())\n",
        "for i, j in enumerate(np.unique(y_set)):\n",
        "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
        "                c = ListedColormap(('red', 'green'))(i), label = j)\n",
        "plt.title('Logistic Regression (Test set)')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Estimated Salary')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCz1eqIVrr4-"
      },
      "source": [
        "## (두번째 실습) 직접구현\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1T2tx4nejtN5"
      },
      "source": [
        "**구현과정에서 사용된 함수의 설명**\n",
        "\n",
        "아래의 코드셀에서는 np.mean(), np.log(), np.exp(), np.dot()와 같은 함수들이 사용된다.np.mean(), np.log(), np.exp()는 이름만으로 기능을 유추할 수 있을 것인데, 아래의 코드셀의 실행 결과를 살펴보자.\n",
        "1. np.mean()은 입력으로 리스트가 주어졌을 때 리스트를 구성하는 원소들의 평균값을 반환한다.\n",
        "2. np.log()에 대해서는 크게 두 가지를 주의해야 한다. (2-1) numpy의 log()는 자연상수(e)를 밑으로 하는 로그를 계산한 것을 반환한다. 즉 numpy의 log(x)는 ln(x)와 동일한 기능을 수행한다. (2-2) numpy의 log()는 리스트를 인자로 받을 수 있다. 리스트가 인자로 주어지면 리스트를 구성하는 개별 원소에 대한 자연 로그 값을 반환한다.\n",
        "3. np.exp()는 지수함수(exponential function), 그 중에서도 e^(x) 형태를 계산한다. np.log()와 마찬가지로 리스트를 인자로 받을 수 있다.\n",
        "4. np.dot()은 좀 더 자세히 살펴보도록 하자.\n",
        "\n",
        "**np.dot()의 동작**\n",
        "\n",
        "아래 코드셀에서\n",
        "\n",
        "1. \"- 1번 상황 : 벡터와 벡터\"은 x = [2, 3, 4]와 w = [1, 2, 3]의 내적 연산 결과를 반환한다.\n",
        "2. \"- 2번 상황 : 행렬과 벡터\"은 x2 = np.array([[1,2], [3, 4], [5, 6]])와 w = [1, 2, 3]에 대하여 np.dot(x2.T, w)의 연산을 수행한다. x2.T는 행렬에 대한 전치연산이므로 (3,2)의 모양을 가지고 있던 x2가 (2, 3) 모양으로 변한다. np.dot(x2.T, w)의 연산 결과인 [22 28]은 (2, 3) 형태인 x2.T와 (3, )형태인 w에 대하여 행렬 곱 연산을 수행한 결과이므로 (2, )형태의 결과물이 출력된다.보다 자세하게 설명하면 [1, 3, 5]벡터와 [1, 2, 3] 벡터의 내적 연산 결과인 22, [2, 4, 6]벡터와 [1, 2, 3]벡터의 내적 연산 결과인 28이 각각 출력되었다.\n",
        "3. \"- 3번 상황 : 행렬과 행렬\"은 (2, 3) 모양인 x2.T와 (3, 2) 모양인 w2에 대하여 행렬곱을 수행한 상황이다. 따라서 (2, 2) 모양의 결과가 계산되었다. 보다 자세하게 설명하면 [1, 3, 5]벡터와 [1, 2, 2] 벡터의 내적 연산 결과인 17, [1, 3, 5]벡터와 [-1, 0, 3] 벡터의 내적 연산 결과인 14, [2, 4, 6]벡터와 [1, 2, 2]벡터의 내적 연산 결과인 22, [2, 4, 6]벡터와 [-1, 0, 3] 벡터의 내적 연산 결과인 16이 출력되었다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ls1C_PrRspOr"
      },
      "source": [
        "**gradients 함수의 구현**\n",
        "\n",
        "```\n",
        "def gradients(X, y, y_hat):\n",
        "    dw = np.dot(X.T, (y-y_hat))\n",
        "    return dw\n",
        "```\n",
        "gradients 함수의 구현을 자세히 살펴보자. train()함수 내부에서 gradients함수가 호출되는 아래의 코드에서 gradients함수에게 인자로 주어지는 xb, yb, y_hat은 각각 (100,2), (100,1), (100,1)의 모양을 하고 있다.\n",
        "\n",
        "```\n",
        "dw = gradients(xb, yb, y_hat)\n",
        "```\n",
        "\n",
        "gradients 함수의 함수 몸체에서는\n",
        "\n",
        "```\n",
        "np.dot(X.T, (y-y_hat))\n",
        "```\n",
        "상기 형태의 연산을 수행하고 있는데, X.T는 (2, 100) 모양의 행렬이고, (y-y_hat)은 (100, 1) 모양의 행렬(혹은 벡터)이므로 np.dot(X.T, (y-y_hat))은 행렬과 벡터를 곱한 (2, 1)모양의 결과물을 반환한다.이 결과물은 \"N인공지능입문_로지스틱회귀p2.pdf\"라는 이름의 강의자료에서 13 슬라이드나 16번 슬라이드의 편미분 수식 중 합에 해당하는 부분만을 구현한 것이다(주의 y_hat은 \"y_hat = sigmoid(np.dot(xb, w)\"의 코드로 계산됨).\n",
        "\n",
        "-정리 : \"np.dot(X.T, (y-y_hat))\"의 간단한 표현으로\n",
        "16번 슬라이드의 summation에 해당하는 복잡한 연산이 표현되었음에 주의하자.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwiS4f2WyltI"
      },
      "source": [
        "**실습 코드에서 주의할 점**\n",
        "\n",
        "```\n",
        "def loss(y, y_hat):\n",
        "    loss = np.mean(y*(np.log(y_hat)) + (1-y)*np.log(1-y_hat))\n",
        "    return -loss\n",
        "```\n",
        "상기 코드로 표현된 loss함수의 구현을 보면 np.mean()을 수행하여 Cross entropy의 평균을 계산함을 알 수 있다.\n",
        "\n",
        "그러나 경사하강법을 적용하여 인자를 갱신하는 다음의 코드를 보면\n",
        "\n",
        "```\n",
        "y_hat = sigmoid(np.dot(xb, w) )\n",
        "            dw = gradients(xb, yb, y_hat)\n",
        "            w += lr*dw\n",
        "```\n",
        "gradients()함수의 반환값에 학습율(lr)을 곱한 값을 w에 더하는 것을 알 수 있다.\n",
        "\n",
        "즉, 실습 코드에서서는 값이 줄어드는 것을 보여주기 위하여 loss 함수에서는 바이너리 크로스 엔트로피의 평균을 계산하고 있으나, 인자를 갱신하는 과정에서는 log likelihood를 사용하는 다소 복잡한 구현을 택하고 있다.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4llDMuHkPCw"
      },
      "outputs": [],
      "source": [
        "x = [2, 3, 4]\n",
        "print(\"=== numpy의 mean() ====\")\n",
        "print(np.mean(x))\n",
        "print(\"=== numpy의 log() ====\")\n",
        "print(np.log(x))\n",
        "print(\"=== numpy의 exp() ====\")\n",
        "print(np.exp(x))\n",
        "print(\"\\n**** numpy의 dot() ****\")\n",
        "\n",
        "print(\"- 1번 상황 : 벡터와 벡터\")\n",
        "w= [1, 2, 3]\n",
        "print(np.dot(np.array(x).T, w))\n",
        "print(\"- 2번 상황 : 행렬과 벡터\")\n",
        "x2 = np.array([[1,2], [3, 4], [5, 6]])\n",
        "print(np.dot(x2.T, w))\n",
        "print(\"- 3번 상황 : 행렬과 행렬\")\n",
        "w2 = [[1, -1], [2, 0], [2,3]]\n",
        "print(np.dot(x2.T, w2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8iQ6ZsmreT6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# (1) (데이터 측면) 데이터 불러오기 (Importing the datas) 및 스케일링\n",
        "dataset = pd.read_csv('Social_Network_Ads.csv')\n",
        "X = dataset.iloc[:, [2, 3]].values\n",
        "y = dataset.iloc[:, 4].values\n",
        "\n",
        "# 특성 스케일링\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X = sc.fit_transform(X)\n",
        "# StandardScaler의 효과를 이해할 수 있도록\n",
        "# X_train = sc.fit_transform(X_train),X_test = sc.transform(X_test)를 주석 처리한 후 결과를 비교해보자.\n",
        "# 주의 : fit_trainsform과 transform의 효과를 비교하려면 현재 구현과 틀리게 train_test_split을 먼저 실행한 후\n",
        "# StandardScaler를 실행해야 한다.\n",
        "\n",
        "# (2) (데이터 측면) 훈련 데이터셋과 테스트 데이터셋을 구분하기\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 1)\n",
        "\n",
        "print(X_train.shape)\n",
        "\n",
        "# (3) (모델 측면) 로지스틱 회귀 모델 불러오기\n",
        "\n",
        " ## 아래 코드는 위의 사이킷런 라이브러리에서 fit, predict 함수들로 4줄만에 표현 가능한 인공지능 분류 모델의 학습과 검증 절차를 세분화하여 파이썬코드로 하나하나 자세히 구현한 내용이다.\n",
        "# 시그모이드 함수의 구현\n",
        "def sigmoid(x):\n",
        "    return 1.0/(1 + np.exp(-x))\n",
        "  ## exp(x)가 지수함수(e^(x))와 동일한 표현임에 유의의.\n",
        "\n",
        "# 로지스틱 회귀의 손실함수형태와 코드를 잘 비교해보자.\n",
        "# 질문 : loss(y, y_hat) 함수에서 계산하는 값은 무엇인가?\n",
        "# 아래 코드에서 y_hat은 y_hat = sigmoid(np.dot(xb, w) )로 계산되는 값으로\n",
        "# C1클래스에 대한 사후확률을 아래 코드에서는 y_hat으로 표현했음을 알 수 있다.\n",
        "def loss(y, y_hat):\n",
        "    loss = np.mean(y*(np.log(y_hat)) + (1-y)*np.log(1-y_hat))\n",
        "    ## np.mean이 (N개의 각 데이터의 계산의 총합/N)을 나타낸다.\n",
        "    return -loss\n",
        "    ## 왜 (-)음수인가? 우도(y_hat)을 증가시키려면, -ln 마이너스 자연로그를 감소시켜야한다.\n",
        "    ## y=0, y=1일때 두가지 상황을 한꺼번에 계산할 수 있는 베르누이 공식으로부터 유도된\n",
        "    ## 바이너리 크로스 엔트로피 손실함수이다. BCE_Loss\n",
        "\n",
        "# 로지스틱 회귀의 인자 갱신식과 잘 비교해보자.\n",
        "def gradients(X, y, y_hat):\n",
        "    dw = np.dot(X.T, (y-y_hat))\n",
        "    return dw\n",
        "    ## 강의자료료의 그라디언트(미분, 기울기) 전개식을 비교해보라. x와 오차(y-y_hat)간의 곱으로 나타낸다.\n",
        "\n",
        "# (4) (학습 과정) 모델 학습 하기\n",
        "# 학습과정에서는 Train 데이터만! 사용된다.\n",
        "def train(X_train, y_train, bs, epochs, lr):\n",
        "    m, n = X_train.shape\n",
        "    w = np.zeros((n,1))\n",
        "    y_train = y_train.reshape(m,1)\n",
        "    losses = []\n",
        "    train_acc = []\n",
        "    test_acc = []\n",
        "    ## 먼저 위와 같이 학습을 위해 필요한 변수들을 정의한다.\n",
        "    # epochs에서 지정된 값만큼 반복.\n",
        "    for epoch in range(epochs):\n",
        "        # 아래 for loop는 어떤 동작을 하는지 설명을 시도해보자\n",
        "        # bs의 값과 m의 값을 파악한 후 xb, yb를 유추하면\n",
        "        # 아래 for loop의 동작을 이해할 수 있다.\n",
        "        for i in range((m-1)//bs + 1): # \"//\"연산자는 파이썬에서 어떤 기능을 하는가?\n",
        "            start_i = i*bs\n",
        "            end_i = start_i + bs\n",
        "            xb = X_train[start_i:end_i]\n",
        "            yb = y_train[start_i:end_i]\n",
        "            y_hat = sigmoid(np.dot(xb, w) )\n",
        "            dw = gradients(xb, yb, y_hat)\n",
        "            w += lr*dw   # 인자 갱신식에서 w = w + lr*dw로 구현되었음에 유의하자\n",
        "\n",
        "# 반복문 for문의 안쪽이 있는 함수는 무엇이고 바깥쪽에 있는 함수는 무엇인지에 유의하자.\n",
        "\n",
        "        l = loss(y_train, sigmoid(np.dot(X_train, w)))\n",
        "        tr_acc = accuracy_score(y_train, predict(X_train, w))\n",
        "        te_acc = accuracy_score(y_test, predict(X_test, w))\n",
        "        losses.append(l)\n",
        "        train_acc.append(tr_acc)\n",
        "        test_acc.append(te_acc)\n",
        "        if epoch %10 == 0:\n",
        "            ## 반복횟수 에포크 10을 나누었을때 나머지가 0인 숫자, 즉 10번째마다 if문이 아래 print()를 출력한다.\n",
        "          print(\"loss = \", l)\n",
        "          print(\"training accuracy \", tr_acc)\n",
        "          print(\"testing accuracy \", te_acc)\n",
        "\n",
        "    return w,losses, train_acc, test_acc\n",
        "    ## 리턴으로 필요한 값을 전달한다.\n",
        "\n",
        "# (5) (성능 평가) 학습된 모델을 대상으로 테스트 데이터로 평가하기. (Predicting the Test set results)\n",
        "# 성능평가 과정에서는 Test 데이터만! 사용된다.\n",
        "# 테스트 데이터에 대하여 예측값 확보\n",
        "# 이번 예제에서는 Train 함수 안에서 사용되었음에 주의\n",
        "def predict(X, w):\n",
        "\n",
        "    # 예측값 계산\n",
        "    preds = sigmoid(np.dot(X, w))\n",
        "\n",
        "    pred_class = []\n",
        "    # 이진 분류에서 사후 확률에 기반한 결정 규칙 구현\n",
        "    pred_class = [1 if i > 0.5 else 0 for i in preds]\n",
        "    ## .sigmoid(np.dot(X, w))의 계산결과가 1/2이상이면 1번 클래스, 그렇지 않으면 0번 클래스로 간주\n",
        "\n",
        "    return np.array(pred_class)\n",
        "\n",
        "# X_train의 shape을 확인해보면 (300,2)이다.\n",
        "w, l, train_acc, test_acc = train(X_train, y_train, bs=100, epochs=100, lr=0.1/len(X_train))\n",
        "## 바이어스, 반복횟수, 알파(학습률) 전부다 직접 정의해준다.\n",
        "## 이렇게 사용자가 직접 바꾸는 초매개변수를 하이퍼파라미터라고 부른다.\n",
        "y_pred = predict(X_test, w)\n",
        "\n",
        "print(\"======== Classification performance ==============\")\n",
        "#분류 성능 측도\n",
        "print(sklearn.metrics.accuracy_score(y_test, y_pred))\n",
        "print(sklearn.metrics.f1_score(y_test, y_pred))\n",
        "plt.plot([i for i in range(1, 101)], train_acc, label = \"Training Accuracy\")\n",
        "plt.plot([i for i in range(1, 101)], test_acc, label = \"Testing Accuracy\")\n",
        "plt.legend()\n",
        "plt.savefig('train_test.png', dpi=300)  ## dpi 해상도\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9jUMxfXv0z4"
      },
      "outputs": [],
      "source": [
        "# Negative log likelihood 그림\n",
        "plt.plot([i for i in range(1, 101)], l, label = \"Negative Log Likelihood\")\n",
        "plt.legend()\n",
        "plt.savefig('Negative_log_likelihood.png', dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V92HQ1ny7hMn"
      },
      "outputs": [],
      "source": [
        "### 훈련 결과 가시화\n",
        "from matplotlib.colors import ListedColormap\n",
        "X_set, y_set = X_train, y_train\n",
        "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
        "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
        "##\n",
        "##\n",
        "\n",
        "predict = np.array([np.dot(x, w) for x in np.array([X1.ravel(), X2.ravel()]).T]).reshape(X1.shape)\n",
        "\n",
        "\n",
        "plt.contourf(X1, X2, np.array([sigmoid(np.dot(x, w)) for x in np.array([X1.ravel(), X2.ravel()]).T]).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
        "plt.xlim(X1.min(), X1.max())\n",
        "plt.ylim(X2.min(), X2.max())\n",
        "for i, j in enumerate(np.unique(y_set)):\n",
        "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
        "                c = ListedColormap(('red', 'green'))(i), label = j)\n",
        "plt.title('Logistic Regression (Training set)')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Estimated Salary')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjfd0aRqzWg0"
      },
      "outputs": [],
      "source": [
        "### 테스트결과 가시화\n",
        "from matplotlib.colors import ListedColormap\n",
        "X_set, y_set = X_test, y_test\n",
        "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
        "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
        "plt.contourf(X1, X2, np.array([sigmoid(np.dot(x, w)) for x in np.array([X1.ravel(), X2.ravel()]).T]).reshape(X1.shape),\n",
        "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
        "plt.xlim(X1.min(), X1.max())\n",
        "plt.ylim(X2.min(), X2.max())\n",
        "for i, j in enumerate(np.unique(y_set)):\n",
        "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
        "                c = ListedColormap(('red', 'green'))(i), label = j)\n",
        "plt.title('Logistic Regression (Test set)')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Estimated Salary')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvWHR742Prmc"
      },
      "outputs": [],
      "source": [
        "x = np.arange(1, 10)\n",
        "y = x.reshape(-1, 1)\n",
        "h = x * y\n",
        "\n",
        "cs = plt.contourf(h, levels=[5, 40, 50],\n",
        "    colors=['#808080', '#A0A0A0', '#C0C0C0'], extend='both')\n",
        "\n",
        "## 컨투어에프 함수의 사용법에 대해서 예시를 들어주는 내용이다.\n",
        "## https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.contourf.html 참고하라.\n",
        "cs.cmap.set_over('red')\n",
        "cs.cmap.set_under('blue')\n",
        "cs.changed()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}