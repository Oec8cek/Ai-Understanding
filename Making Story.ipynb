{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "beac2840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Licensed to the Apache Software Foundation (ASF) under one\n",
    "# or more contributor license agreements.  See the NOTICE file\n",
    "# distributed with this work for additional information\n",
    "# regarding copyright ownership.  The ASF licenses this file\n",
    "# to you under the Apache License, Version 2.0 (the\n",
    "# \"License\"); you may not use this file except in compliance\n",
    "# with the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing,\n",
    "# software distributed under the License is distributed on an\n",
    "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "# KIND, either express or implied.  See the License for the\n",
    "# specific language governing permissions and limitations\n",
    "# under the License.\n",
    "\"\"\"GPT models.\"\"\"\n",
    "\n",
    "__all__ = ['GPT2Model', 'GPT2SelfAttentionLayer', 'GPT2FFNLayer',\n",
    "           'gpt2_117m', 'gpt2_345m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85b7e817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet.gluon import HybridBlock, nn\n",
    "from mxnet.gluon.model_zoo import model_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36a5b8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonnlp.base import get_home_dir\n",
    "from gluonnlp.model.attention_cell import DotProductAttentionCell\n",
    "from gluonnlp.model.block import GELU\n",
    "from gluonnlp.model.utils import _load_pretrained_params, _load_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f00459e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2SelfAttentionLayer(HybridBlock):\n",
    "    \"\"\"Self-attention layer used in OpenAI GPT-2.\n",
    "    Parameters\n",
    "    ----------\n",
    "    units : int\n",
    "        Number of units for the output.\n",
    "    num_heads : int\n",
    "        Number of heads in multi-head attention\n",
    "    dropout : float\n",
    "        Dropout probability of the attention probabilities.\n",
    "    weight_initializer : str or Initializer\n",
    "        Initializer for the input weights matrix, used for the linear\n",
    "        transformation of the inputs.\n",
    "    bias_initializer : str or Initializer\n",
    "        Initializer for the bias vector.\n",
    "    prefix : str, default None.\n",
    "        Prefix for name of `Block`s. (and name of weight if params is `None`).\n",
    "    params : Parameter or None\n",
    "        Container for weight sharing between cells. Created if `None`.\n",
    "    Inputs:\n",
    "        - **inputs** : input sequence of shape (batch_size, length, in_dim).\n",
    "        - **states** : None, or list of tensors\n",
    "            The states, for initial states and masks that contains\n",
    "            the previous encoded key/values\n",
    "            prev_key (batch_size, num_heads, past_length, mem_length),\n",
    "            prev_value (batch_size, num_heads, past_length, mem_length)\n",
    "            None means no previous states.\n",
    "    Outputs:\n",
    "        - **outputs** : output encoding of shape (batch_size, length, C_out).\n",
    "        - **additional_outputs** : list of tensors.\n",
    "            Either be an empty list or contains the attention weights in this step.\n",
    "            The attention weights will have shape (batch_size, num_heads, length, mem_length)\n",
    "    \"\"\"\n",
    "    def __init__(self, units, num_heads, dropout=0.0,\n",
    "                 weight_initializer=mx.init.Normal(0.02), bias_initializer='zeros',\n",
    "                 prefix=None, params=None):\n",
    "        super(GPT2SelfAttentionLayer, self).__init__(prefix=prefix, params=params)\n",
    "        self._units = units\n",
    "        self._num_heads = num_heads\n",
    "        assert units % num_heads == 0\n",
    "        with self.name_scope():\n",
    "            self._multi_head_qkv_proj = nn.Dense(units=units * 3, flatten=False, use_bias=True,\n",
    "                                                 weight_initializer=weight_initializer,\n",
    "                                                 bias_initializer=bias_initializer,\n",
    "                                                 prefix='qkv_proj_')\n",
    "            self._base_attn_cell = DotProductAttentionCell(\n",
    "                scaled=True, dropout=dropout, prefix='attn_')\n",
    "            self._dropout_layer = nn.Dropout(dropout)\n",
    "            self._out_proj = nn.Dense(units=units, flatten=False, use_bias=True,\n",
    "                                      weight_initializer=weight_initializer,\n",
    "                                      bias_initializer=bias_initializer,\n",
    "                                      prefix='out_proj_')\n",
    "\n",
    "    def hybrid_forward(self, F, data, states=None):  # pylint: disable=arguments-differ\n",
    "        # Generate mask\n",
    "        if states is not None:\n",
    "            prev_key, prev_value = states\n",
    "\n",
    "            prev_len_range = F.contrib.arange_like(prev_key, axis=2)\n",
    "            data_len_range = F.contrib.arange_like(data, axis=1)\n",
    "            prev_len = F.broadcast_add(F.slice_axis(prev_len_range, axis=0, begin=-1, end=None),\n",
    "                                       F.ones((1, )))\n",
    "\n",
    "            data_pos = F.broadcast_add(F.contrib.arange_like(data, axis=1), prev_len)\n",
    "            all_pos = F.contrib.arange_like(F.concat(prev_len_range, data_len_range, dim=0))\n",
    "        else:\n",
    "            prev_key, prev_value = None, None\n",
    "            data_pos = F.contrib.arange_like(data, axis=1)\n",
    "            all_pos = data_pos\n",
    "\n",
    "        mask = F.broadcast_lesser_equal(all_pos.reshape((1, -1)), data_pos.reshape((-1, 1)))\n",
    "        mask = F.broadcast_like(F.expand_dims(mask, axis=0), data, lhs_axes=(0, ), rhs_axes=(0, ))\n",
    "        mask = F.concat(*[mask] * self._num_heads, dim=0)\n",
    "\n",
    "        # Multi-head attention\n",
    "        qkv = self._multi_head_qkv_proj(data)  # Shape (batch_size, seq_len, 3 * units)\n",
    "        qkv = F.swapaxes(qkv, 1, 2)  # Shape (batch_size, 3 * units, seq_len)\n",
    "\n",
    "        # Each has shape (batch_size, units, seq_len)\n",
    "        query, key, value = F.split(qkv, num_outputs=3, axis=1)\n",
    "        # Map each to have shape (batch_size * num_head, ele_units, seq_len)\n",
    "        query = query.reshape(shape=(0, -4, self._num_heads, -1, 0)).reshape(\n",
    "            shape=(-1, 0, 0), reverse=True)\n",
    "        key = key.reshape(shape=(0, -4, self._num_heads, -1, 0)).reshape(\n",
    "            shape=(-1, 0, 0), reverse=True)\n",
    "        value = value.reshape(shape=(0, -4, self._num_heads, -1, 0)).reshape(\n",
    "            shape=(-1, 0, 0), reverse=True)\n",
    "        query = F.swapaxes(query, 1, 2)\n",
    "        key = F.swapaxes(key, 1, 2)\n",
    "        value = F.swapaxes(value, 1, 2)\n",
    "        if prev_key is not None:\n",
    "            # Shape (batch_size * num_heads, all_len, ele_units)\n",
    "            key = F.concat(prev_key.reshape((-1, 0, 0), reverse=True), key, dim=1)\n",
    "        if prev_value is not None:\n",
    "            value = F.concat(prev_value.reshape((-1, 0, 0), reverse=True),\n",
    "                             value, dim=1)\n",
    "\n",
    "        # Shape (batch_size * num_heads, all_len, ele_units)\n",
    "        out, _ = self._base_attn_cell(query, key, value, mask)\n",
    "        out = F.transpose(out.reshape((-1, self._num_heads, 0, 0), reverse=True),\n",
    "                          axes=(0, 2, 1, 3)).reshape((0, 0, -1))\n",
    "        out = self._out_proj(out)\n",
    "        out = self._dropout_layer(out)\n",
    "        return out, [key.reshape((-1, self._num_heads, 0, 0), reverse=True),\n",
    "                     value.reshape((-1, self._num_heads, 0, 0), reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35c905ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2FFNLayer(HybridBlock):\n",
    "    \"\"\"Feed-forward network (FFN) layer used in OpenAI GPT-2.\n",
    "    Parameters\n",
    "    ----------\n",
    "    units : int\n",
    "        Number of units for the output.\n",
    "    hidden_size : int\n",
    "        number of units in the hidden layer of position-wise feed-forward networks\n",
    "    weight_initializer : str or Initializer\n",
    "        Initializer for the input weights matrix, used for the linear\n",
    "        transformation of the inputs.\n",
    "    bias_initializer : str or Initializer\n",
    "        Initializer for the bias vector.\n",
    "    num_heads : int\n",
    "        Number of heads in multi-head attention\n",
    "    dropout : float\n",
    "        Dropout probability of the attention probabilities.\n",
    "    prefix : str, default None.\n",
    "        Prefix for name of `Block`s. (and name of weight if params is `None`).\n",
    "    params : Parameter or None\n",
    "        Container for weight sharing between cells. Created if `None`.\n",
    "    Inputs:\n",
    "        - **inputs** : input sequence of shape (batch_size, length, C_in)\n",
    "    Outputs:\n",
    "        - **outputs** : the output of the encoder. Shape is (batch_size, length, C_out)\n",
    "    \"\"\"\n",
    "    def __init__(self, units, hidden_size, dropout=0.0,\n",
    "                 weight_initializer=mx.init.Normal(0.02), bias_initializer='zeros',\n",
    "                 prefix=None, params=None):\n",
    "        super(GPT2FFNLayer, self).__init__(prefix=prefix, params=params)\n",
    "        self._units = units\n",
    "        self._hidden_size = hidden_size\n",
    "        with self.name_scope():\n",
    "            self._hidden_map = nn.Dense(flatten=False, units=hidden_size,\n",
    "                                        weight_initializer=weight_initializer,\n",
    "                                        bias_initializer=bias_initializer)\n",
    "            self._out_map = nn.Dense(flatten=False, units=units,\n",
    "                                     weight_initializer=weight_initializer,\n",
    "                                     bias_initializer=bias_initializer)\n",
    "            # self._act = GELU(approximate=False)\n",
    "            self._act = GELU()\n",
    "            self._dropout_layer = nn.Dropout(dropout)\n",
    "\n",
    "    def hybrid_forward(self, F, data): # pylint: disable=arguments-differ\n",
    "        out = self._out_map(self._act(self._hidden_map(data)))\n",
    "        return self._dropout_layer(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6555dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Model(HybridBlock):\n",
    "    \"\"\"Generic Model for GPT-2.\n",
    "    Parameters\n",
    "    ----------\n",
    "    units : int\n",
    "        Number of units for the output.\n",
    "    vocab_size : int or None, default None\n",
    "        The size of the vocabulary.\n",
    "    max_length : int\n",
    "        Maximum length of the input sequence\n",
    "    num_layers : int\n",
    "        Number of attention layers.\n",
    "    num_heads : int\n",
    "        Number of heads in multi-head attention\n",
    "    dropout : float\n",
    "        Dropout probability of the attention probabilities.\n",
    "    prefix : str, default None.\n",
    "        Prefix for name of `Block`s. (and name of weight if params is `None`).\n",
    "    params : Parameter or None\n",
    "        Container for weight sharing between cells. Created if `None`.\n",
    "    \"\"\"\n",
    "    def __init__(self, units, vocab_size, max_length, num_layers, num_heads, dropout=0.0,\n",
    "                 prefix=None, params=None):\n",
    "        super(GPT2Model, self).__init__(prefix=prefix, params=params)\n",
    "        self._units = units\n",
    "        self._max_length = max_length\n",
    "        self._num_layers = num_layers\n",
    "        self._num_heads = num_heads\n",
    "        with self.name_scope():\n",
    "            self._pos_embed = nn.Embedding(input_dim=max_length, output_dim=units,\n",
    "                                           weight_initializer=mx.init.Normal(0.01),\n",
    "                                           prefix='pos_embed_')\n",
    "            self._embed = nn.Embedding(input_dim=vocab_size, output_dim=units, prefix='embed_',\n",
    "                                       weight_initializer=mx.init.Normal(0.02))\n",
    "            self._drop = nn.Dropout(dropout)\n",
    "            self._logits_proj = nn.Dense(units=vocab_size, in_units=units, use_bias=False,\n",
    "                                         flatten=False, params=self._embed.params)\n",
    "            self._self_attention_layers = nn.HybridSequential()\n",
    "            self._ffn_layers = nn.HybridSequential()\n",
    "            self._attn_ln = nn.HybridSequential()\n",
    "            self._ffn_ln = nn.HybridSequential()\n",
    "            for i in range(num_layers):\n",
    "                self._self_attention_layers.add(GPT2SelfAttentionLayer(\n",
    "                    units=units, num_heads=num_heads, dropout=dropout,\n",
    "                    prefix='self_attn{}_'.format(i)))\n",
    "                self._ffn_layers.add(GPT2FFNLayer(\n",
    "                    units=units, hidden_size=units * 4, dropout=dropout, prefix='ffn{}_'.format(i)))\n",
    "                self._attn_ln.add(nn.LayerNorm(prefix='attn_ln{}_'.format(i)))\n",
    "                self._ffn_ln.add(nn.LayerNorm(prefix='ffn_ln{}_'.format(i)))\n",
    "                self._final_ln = nn.LayerNorm(prefix='final_ln{}_'.format(i))\n",
    "\n",
    "    def hybrid_forward(self, F, data, states=None): # pylint: disable=arguments-differ\n",
    "        \"\"\"Compute\n",
    "        Notes\n",
    "        -----\n",
    "        If you hybridized the GPT2Model by calling net.hybridize(), you cannot\n",
    "        switch between states=None, and states=list_of_NDArray between calls to\n",
    "        the net. The hybridized model will only support the type of states used\n",
    "        during the first call after hybridization.\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : NDArray\n",
    "            Shape (batch_size, seq_len)\n",
    "        states : list of NDArray or None\n",
    "        Returns\n",
    "        -------\n",
    "        out : NDArray\n",
    "            Shape (batch_size, seq_len, vocab_size)\n",
    "        new_states : list of NDArray\n",
    "        \"\"\"\n",
    "        new_states = []\n",
    "        if states is not None:\n",
    "            prev_len_range = F.contrib.arange_like(states[0], axis=2).astype('int32')\n",
    "            prev_len = F.broadcast_add(F.slice_axis(prev_len_range, axis=0, begin=-1, end=None),\n",
    "                                       F.ones((1, ), dtype='int32'))\n",
    "            data_pos = F.broadcast_add(\n",
    "                F.contrib.arange_like(data, axis=1).astype('int32'), prev_len)\n",
    "        else:\n",
    "            data_pos = F.contrib.arange_like(data, axis=1).astype('int32')\n",
    "        if F is mx.nd:\n",
    "            length = data.shape[1] + (states[0].shape[2] if states is not None else 0)\n",
    "            assert length <= self._max_length\n",
    "        # astype cast to workaround https://github.com/apache/incubator-mxnet/issues/16851\n",
    "        data_pos = F.broadcast_like(F.expand_dims(data_pos, axis=0), data.astype('int32'),\n",
    "                                    lhs_axes=(0, ), rhs_axes=(0, ))\n",
    "        out = self._drop(self._embed(data) + self._pos_embed(data_pos))\n",
    "        for i in range(self._num_layers):\n",
    "            attn_layer = self._self_attention_layers[i]\n",
    "            ffn_layer = self._ffn_layers[i]\n",
    "            attn_ln = self._attn_ln[i]\n",
    "            ffn_ln = self._ffn_ln[i]\n",
    "            layer_states = None if states is None else states[2*i:(2*i + 2)]\n",
    "            h, new_layer_states = attn_layer(attn_ln(out), layer_states)\n",
    "            out = out + h\n",
    "            h = ffn_layer(ffn_ln(out))\n",
    "            out = out + h\n",
    "            new_states.extend(new_layer_states)\n",
    "        out = self._final_ln(out)\n",
    "        logits = self._logits_proj(out)\n",
    "        return logits, new_states\n",
    "\n",
    "    def state_info(self, *args, **kwargs): # pylint: disable=unused-argument\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cb5df20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_store._model_sha1.update(\n",
    "    {name: checksum for checksum, name in [\n",
    "        ('26416f2ec2ab0c5f37e74dcec801f3e659546e03', 'gpt2_117m_openai_webtext'),\n",
    "        ('29173e25d2f3b187745bea6689693bb771862f81', 'gpt2_345m_openai_webtext'),\n",
    "    ]})\n",
    "\n",
    "gpt2_117m_hparams = {\n",
    "    'units': 768,\n",
    "    'max_length': 1024,\n",
    "    'num_heads': 12,\n",
    "    'num_layers': 12,\n",
    "    'dropout': 0.0,\n",
    "}\n",
    "\n",
    "gpt2_345m_hparams = {\n",
    "    'units': 1024,\n",
    "    'max_length': 1024,\n",
    "    'num_heads': 16,\n",
    "    'num_layers': 24,\n",
    "    'dropout': 0.0,\n",
    "}\n",
    "\n",
    "gpt2_hparams = {\n",
    "    'gpt2_117m': gpt2_117m_hparams,\n",
    "    'gpt2_345m': gpt2_345m_hparams,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c06f47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt2_117m(dataset_name=None, vocab=None, pretrained=True, ctx=mx.cpu(),\n",
    "              root=os.path.join(get_home_dir(), 'models'), **kwargs):\n",
    "    \"\"\"Generic GPT-2 model.\n",
    "    The number of layers (L) is 12, number of units (H) is 768, and the\n",
    "    number of self-attention heads (A) is 12.\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_name : str or None, default None\n",
    "        If not None, the dataset name is used to load a vocabulary for the\n",
    "        dataset. If the `pretrained` argument is set to True, the dataset name\n",
    "        is further used to select the pretrained parameters to load.\n",
    "        Options include 'book_corpus_wiki_en_uncased' and 'book_corpus_wiki_en_cased'.\n",
    "    vocab : gluonnlp.vocab.BERTVocab or None, default None\n",
    "        Vocabulary for the dataset. Must be provided if dataset_name is not\n",
    "        specified. Ignored if dataset_name is specified.\n",
    "    pretrained : bool, default True\n",
    "        Whether to load the pretrained weights for model.\n",
    "    ctx : Context, default CPU\n",
    "        The context in which to load the pretrained weights.\n",
    "    root : str, default '$MXNET_HOME/models'\n",
    "        Location for keeping the model parameters.\n",
    "        MXNET_HOME defaults to '~/.mxnet'.\n",
    "    Returns\n",
    "    -------\n",
    "    GPT2Model, gluonnlp.vocab.Vocab\n",
    "    \"\"\"\n",
    "    return _get_gpt2_model('gpt2_117m', dataset_name=dataset_name, vocab=vocab,\n",
    "                           pretrained=pretrained, ctx=ctx, root=root,\n",
    "                           **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "329b5826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt2_345m(dataset_name=None, vocab=None, pretrained=True, ctx=mx.cpu(),\n",
    "              root=os.path.join(get_home_dir(), 'models'), **kwargs):\n",
    "    \"\"\"Generic GPT-2 model.\n",
    "    The number of layers (L) is 24, number of units (H) is 1024, and the\n",
    "    number of self-attention heads (A) is 24.\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_name : str or None, default None\n",
    "        If not None, the dataset name is used to load a vocabulary for the\n",
    "        dataset. If the `pretrained` argument is set to True, the dataset name\n",
    "        is further used to select the pretrained parameters to load.\n",
    "        Options include 'book_corpus_wiki_en_uncased' and 'book_corpus_wiki_en_cased'.\n",
    "    vocab : gluonnlp.vocab.BERTVocab or None, default None\n",
    "        Vocabulary for the dataset. Must be provided if dataset_name is not\n",
    "        specified. Ignored if dataset_name is specified.\n",
    "    pretrained : bool, default True\n",
    "        Whether to load the pretrained weights for model.\n",
    "    ctx : Context, default CPU\n",
    "        The context in which to load the pretrained weights.\n",
    "    root : str, default '$MXNET_HOME/models'\n",
    "        Location for keeping the model parameters.\n",
    "        MXNET_HOME defaults to '~/.mxnet'.\n",
    "    Returns\n",
    "    -------\n",
    "    GPT2Model, gluonnlp.vocab.Vocab\n",
    "    \"\"\"\n",
    "    return _get_gpt2_model('gpt2_345m', dataset_name=dataset_name, vocab=vocab,\n",
    "                           pretrained=pretrained, ctx=ctx, root=root,\n",
    "                           **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16affe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_gpt2_model(model_name=None, dataset_name=None, vocab=None, pretrained=True, ctx=mx.cpu(),\n",
    "                    root=os.path.join(get_home_dir(), 'models'), **kwargs):\n",
    "    \"\"\"Any predefined GPT-2 model.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name : str or None, default None\n",
    "        Options include 'gpt2_117m' and 'gpt2_345m'.\n",
    "    dataset_name : str or None, default None\n",
    "        If not None, the dataset name is used to load a vocabulary for the\n",
    "        dataset. If the `pretrained` argument is set to True, the dataset name\n",
    "        is further used to select the pretrained parameters to load.\n",
    "        The supported datasets for model_name of either bert_24_1024_16 and\n",
    "        bert_12_768_12 are 'openai_webtext'.\n",
    "    vocab : gluonnlp.vocab.BERTVocab or None, default None\n",
    "        Vocabulary for the dataset. Must be provided if dataset_name is not\n",
    "        specified. Ignored if dataset_name is specified.\n",
    "    pretrained : bool, default True\n",
    "        Whether to load the pretrained weights for model.\n",
    "    ctx : Context, default CPU\n",
    "        The context in which to load the pretrained weights.\n",
    "    root : str, default '$MXNET_HOME/models'\n",
    "        Location for keeping the model parameters.\n",
    "        MXNET_HOME defaults to '~/.mxnet'.\n",
    "    Returns\n",
    "    -------\n",
    "    GPT2Model, gluonnlp.vocab.Vocab\n",
    "    \"\"\"\n",
    "    predefined_args = gpt2_hparams[model_name]\n",
    "    mutable_args = ['dropout']\n",
    "    mutable_args = frozenset(mutable_args)\n",
    "    assert all((k not in kwargs or k in mutable_args) for k in predefined_args), \\\n",
    "        'Cannot override predefined model settings.'\n",
    "    predefined_args.update(kwargs)\n",
    "    vocab = _load_vocab(dataset_name, vocab, root)\n",
    "    # BERT\n",
    "    net = GPT2Model(units=predefined_args['units'],\n",
    "                    vocab_size=len(vocab),\n",
    "                    max_length=predefined_args['max_length'],\n",
    "                    num_layers=predefined_args['num_layers'],\n",
    "                    num_heads=predefined_args['num_heads'],\n",
    "                    dropout=predefined_args['dropout'],\n",
    "                    **kwargs)\n",
    "    if pretrained:\n",
    "        _load_pretrained_params(net, model_name, dataset_name, root, ctx)\n",
    "    return net, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30954359",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
